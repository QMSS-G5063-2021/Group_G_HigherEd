---
title: "Text_to_Map"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r packages, message = FALSE, warning = FALSE, echo=FALSE}
## Run / Install before executing slides

# Load packages.
library(ggplot2)    # the king of plotting 
library(magrittr)   # chain operators, e.g. to "pipe" a value forward
library(stats)
library(manifestoR)
library(readtext)
library(SnowballC)
library(tidyr)
library(tidytext)      # for data manipulation 
library(qdapDictionaries)
library(base64enc)
#install.packages("ggridges",dependencies =TRUE)
library(tidyverse)
library(RColorBrewer)
#install.packages("wesanderson")
library(wesanderson)
library(rtweet)
library(ggmap)
library(sp)
library(leaflet)
library(tm)
library(htmlwidgets)
library(htmltools)
#install.packages("leaflet.extras", dependencies = TRUE)
library(leaflet.extras)
library(zoo)
library(wordcloud)
library(wesanderson)
library(lubridate)
#install webshot
library(webshot)
install_phantomjs()
```

```{r, echo=FALSE}
Tweets_state <- readRDS('./Data/Geotweets_state_cleaned.RDS') %>% 
  mutate(id=row_number())
```

```{r, echo=FALSE}
#source("../../src/visuals/ourtheme.R")

SearchTrend <- read_csv('./Data/SearchTrend.csv', col_names = TRUE, skip = 2) %>% 
  rename(Total_Searches = 'student loan forgiveness: (United States)') %>% 
  mutate(Month = zoo::as.yearmon(Month))

Trend <- ggplot(SearchTrend,aes(x=Month, y=Total_Searches,group=1)) +
  geom_line(color = "#68A982") +
  scale_color_manual(values="#F2C65F")+
  xlab("") + 
  ylab("Total Searches") + 
  labs(title="Interest in #StudentLoanForgiveness over Time",caption = "Source: Google Trend Search") +
  theme(text=element_text(size=12,  family="serif"),
        panel.background= element_rect(fill="white"),
        legend.position = "bottom",
        axis.ticks = element_blank(),
        plot.title = element_text(hjust=0.5))
Trend  
png("Twitter_Trend.png") #Saving plot to png

dev.off()
```

#Convert Dataframe to corpus
```{r, eval= TRUE, echo=FALSE}
# Use tm package convert dataframe to corpus: https://www.rdocumentation.org/packages/tm/versions/0.7-8/topics/DataframeSource
doc_id = c(1:4619)
text_df <- data.frame(doc_id, text = Tweets_state$text, stringsAsFactors = FALSE)

# Convert example_text to a corpus: Success_corpus
tweets_corpus <- VCorpus(DataframeSource(text_df))

corpus <- tm_map(tweets_corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, c(stopwords("en")))
corpus <- tm_map(corpus, content_transformer(function(x) gsub("[[:cntrl:]]", "", x))) #remove control characters 
corpus <- tm_map(corpus, content_transformer(function(x) gsub("http\\S+", "", x))) #remove website addresses
corpus <- tm_map(corpus, content_transformer(function(x) gsub("@[A-Za-z0-9]+", "", x))) #remove mentions in text
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)

#Document Term Matrix of tweets
tweets_dtm <- DocumentTermMatrix(corpus)
```

#Convert DTM to tidy dataframe
```{r, eval= TRUE, echo=FALSE}
tweets_tidy <- tidy(tweets_dtm) %>% 
  mutate(index = as.numeric(document)) %>% 
  left_join(Tweets_state, by=c("index"="id")) %>% 
    mutate(state = ifelse(state=="new york:long island","new york",
                        ifelse(state=="new york:main","new york",
                        ifelse(state=="new york:main","new york",
                        ifelse(state=="new york:manhattan","new york",
                        ifelse(state=="north carolina:main","north carolina",
                        ifelse(state=="massachusetts:main","massachusetts",
                        ifelse(state=="michigan:north","michigan",
                        ifelse(state=="michigan:south","michigan",
                        ifelse(state=="virginia:main", "virginia",
                        ifelse(state=="washington:main","washington",state))))))))))) 
tweets_tidy_wc <- tweets_tidy %>% 
  group_by(term) %>%
  summarize(n = sum(count)) %>%
  arrange(desc(n)) %>% 
  filter(! term %in% c("student","loan","debt","cancelstudentdebt","amp","forgiveness","like","can","just","get","will")) 

 # Create a wordcloud with wesanderson palette
Twitter_wd <- wordcloud(tweets_tidy_wc$term, tweets_tidy_wc$n,
       max.words = 100, colors = wes_palette(name="Royal2"),
       family = "serif")

```

#Get list of top words by state
```{r, eval= TRUE, echo=FALSE}
State_top_words <- tweets_tidy %>% 
  select(term, count, state) %>% 
  group_by(state, term) %>% 
  summarise(total=sum(count)) %>% 
  arrange(desc(total)) %>% 
  filter(! term %in% c("student","loan","debt","cancelstudentdebt","amp")) %>% 
  group_by(state) %>% 
  slice_max(order_by = total, n=15) %>% 
  select(-total) %>% 
  summarise(Terms = list(term))
  
```

```{r, echo=FALSE}
#Geocoded Tweets with selected columns
#saveRDS(tweets_tidy, "TidyTwText.RDS")
```

```{r, eval= TRUE, echo=FALSE}
# Get Bing lexicon
bing <- get_sentiments("bing")

# Join text to lexicon
Tweets_bing <- inner_join(tweets_tidy, bing, by = c("term" = "word")) %>%
   # Count by sentiment, index, document
  count(sentiment,index,document, text) %>%
   # Spread sentiments
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive-negative) 
```

```{r, eval= TRUE, echo=FALSE, }
# Join sentiment score with original twitter data fame
Tweets_sentiment <- Tweets_state %>% 
  inner_join(Tweets_bing, by = c("id" = "index")) %>% 
  select(-c(text.y,document, size=favorite_count)) 

Pos_Neg <- ggplot(Tweets_sentiment, aes(x=positive,y=negative)) +
  geom_jitter(alpha=0.8, color="rosybrown3") +
  xlab("Positive Sentiment") + 
  ylab("Negative Sentiment") + 
  labs(title="Twitter Sentiment Pattern",caption = "Source: Twitter") +
  theme(text=element_text(size=12,  family="serif"),
        panel.background= element_rect(fill="white"),
        legend.position = "bottom",
        axis.ticks = element_blank(),
        plot.title = element_text(hjust=0.5))

png("Pos_Neg.png") #Saving plot to png
print(Pos_Neg)
dev.off()
```

#Letâ€™s make a choropleth map based on sentiment score by state
```{r, eval= TRUE, echo=FALSE}

# count by state
tw_state <- Tweets_sentiment %>%
  mutate(state = ifelse(state=="new york:long island","new york",
                        ifelse(state=="new york:main","new york",
                        ifelse(state=="new york:main","new york",
                        ifelse(state=="new york:manhattan","new york",
                        ifelse(state=="north carolina:main","north carolina",
                        ifelse(state=="massachusetts:main","massachusetts",
                        ifelse(state=="michigan:north","michigan",
                        ifelse(state=="michigan:south","michigan",
                        ifelse(state=="virginia:main", "virginia",
                        ifelse(state=="washington:main","washington",state))))))))))) %>% 
  group_by(state) %>% 
  mutate(N=n(), T.Sentiment=sum(sentiment), WordCounts = sum(display_text_width)) %>%   #get total number of tweets & sum of sentiment score by state
  select(display_text_width, lon, lat, state, T.Sentiment, N, WordCounts) %>%  #lon & lat are vary within each state.Keep first one. 
  group_by(state, lon, lat) %>% 
  summarise(Avg.Sentiment=T.Sentiment/N, WordCounts=WordCounts) %>% #calculate average sentiment score by state and keep total word counts by state
  ungroup(lon,lat) %>% 
  filter(row_number()==1) %>% 
  mutate(lon=ifelse(state=="new york",-73.935242, lon)) %>%  #change coordinates for New York State
  mutate(lat=ifelse(state=="new york",40.730610,lat)) %>% 
  filter(!is.na(state)) %>% 
  left_join(State_top_words, by="state")

# Polygon stuff from shape file
#install.packages("tigris")
library(tigris)
states <- states(cb=T) %>% 
  mutate(name = tolower(NAME))

# Use the Tigris function geo_join to bring together the states shapefile and the tw_states dataframe
states_shape_tweets <- geo_join(states, tw_state, "name", "state")
```

#choropleth map of Twitter Sentiment across States 
```{r, eval= TRUE, echo=FALSE}
# Creating a color palette based on the number range in the total column
pal <- colorNumeric("YlOrRd", domain=states_shape_tweets$Avg.Sentiment)

# Setting up the pop up text
popup_sb <- paste0("State: ", as.character(states_shape_tweets$NAME),"<br/>",
                  "Average Sentiment Score: ", as.character(states_shape_tweets$Avg.Sentiment),"<br/>",
                  "Total Word Count: ",as.character(states_shape_tweets$WordCounts),"<br/>",
                  "Top Words: ",as.character(states_shape_tweets$Terms),"<br/>")

# Mapping it with the new tiles CartoDB.Positron
leaflet() %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  setView(-98.483330, 38.712046, zoom = 4) %>%
  addPolygons(data = states_shape_tweets ,
              fillColor = ~pal(states_shape_tweets$Avg.Sentiment),
              fillOpacity = 0.7,
              weight = 1.5,
              opacity = 1,
              color = "cornsilk",
              dashArray = "3",
              smoothFactor = 0.2,
              popup = ~popup_sb,
              highlight = highlightOptions(weight = 4,
                                           color = "#839EA8",
                                           dashArray = "",
                                           fillOpacity = 0.7,
                                           bringToFront = TRUE),
              popupOptions = popupOptions(style = list("font-weight" = "normal",
                                                       padding = "3px 8px",
                                                       "box-shadow" = "3px 3px rgba(0,0,0,0.25)"),
                                          textsize = "15px",
                                          direction = "auto")) %>%
  addLegend(pal = pal,
            values = states_shape_tweets$Avg.Sentiment,
            position = "bottomleft",
            title = "Average Sentiment Score",
            opacity = 0.7)

```

#Get Locations of Elite/ Selective Schools
```{r, echo=FALSE, warning=FALSE}

Schools <- read_csv('~/Documents/GitHub/DataVisualization/Group_G_HigherEd/src/2010_2019_student_debt.csv') %>%
  select(INSTNM, CITY,LATITUDE, LONGITUDE, ADM_RATE) %>% 
  filter(!is.na(ADM_RATE))

SelSchools <-Schools %>% 
  mutate(Selectivity = case_when(
    ADM_RATE < 0.1 ~ 'elite',
    ADM_RATE < 0.2 ~ 'highly selective',
    ADM_RATE < 0.3 ~ 'selective',
    ADM_RATE < 0.7 ~ 'less selective',
    TRUE ~ 'not selective')) %>% 
  filter(Selectivity %in% c('elite', 'highly selective','selective'), !is.na(LONGITUDE)) %>% 
  distinct(INSTNM, .keep_all=TRUE) 
```


#Twitter Sentiment Map & Institutions'Location Map
```{r, echo=FALSE}
# Creating a color palette based on the number range in the total column
ppal <- colorNumeric("RdYlGn", domain=states_shape_tweets$Avg.Sentiment)

# Creating a color palette based on Selectivity of schools
ppal2 <- colorFactor(palette = c("purple", "forestgreen", "deepskyblue3"), 
               levels = c("elite", "highly selective", "selective"))

icon.pop <- pulseIcons(color = ifelse(SelSchools$Selectivity == "elite", "#E64141", 
                                      ifelse(SelSchools$Selectivity =="highly selective", "#F9C874", "#97D6FF")))

# Setting up the pop up text
popup_sb <- paste0("State: ", as.character(states_shape_tweets$NAME),"<br/>",
                  "Average Sentiment Score: ", as.character(states_shape_tweets$Avg.Sentiment),"<br/>",
                  "Total Word Count: ",as.character(states_shape_tweets$WordCounts),"<br/>",
                  "Top Words: ",as.character(states_shape_tweets$Terms),"<br/>")

popup_sb2 <- paste0("University: ", as.character(SelSchools$INSTNM),"<br/>",
                  "City: ", as.character(SelSchools$CITY),"<br/>",
                  "Admission Rate: ",as.character(SelSchools$ADM_RATE))

getColor <- function(SelSchools) {
  sapply(SelSchools$ADM_RATE, function(ADM_RATE) {
  if(ADM_RATE <= 0.1) {
    "purple"
  } else if(ADM_RATE <= 0.2) {
    "green"
  } else {
    "blue"
  } })
}

icons <- awesomeIcons(
  icon = 'ios-close',
  iconColor = 'black',
  library = 'ion',
  markerColor = getColor(SelSchools)
)


# Map
map_title2 <- tags$p(tags$style('p {color: gray; font-size: 14px; family: serif}'),
                    tags$b('#CancelStudentDebt Tweets in the US - Location of Selective Insitutions'))

leaflet() %>%
  setView(-98.483330, 38.712046, zoom = 4) %>% 
#Base Group
  addTiles(group = "OSM (default)") %>%
  addTiles('http://{s}.basemaps.cartocdn.com/dark_all/{z}/{x}/{y}.png', 
    attribution='Map tiles by <a href="http://stamen.com">Stamen Design</a>, <a href="http://creativecommons.org/licenses/by/3.0">CC BY 3.0</a> &mdash; Map data &copy; <a href="http://www.openstreetmap.org/copyright">OpenStreetMap</a>', group = "Dark") %>%
#Overlay Group  
  # Add Institution Data
  # addPulseMarkers(data = SelSchools,
  #            lng= ~LONGITUDE, lat = ~LATITUDE,
  #            icon = icon.pop,
  #            label = popup_sb2,
  #            group = 'Selective Institutions') %>%
  # addCircles(data = SelSchools,
  #            lng = ~LONGITUDE, lat = ~LATITUDE,
  #            popup=popup_sb2,
  #            radius = SelSchools$ADM_RATE*400, 
  #            color=color_Selectivity,
  #            group = 'Selective Institutions') %>%
  addAwesomeMarkers(data = SelSchools,
                    lng = ~LONGITUDE, lat = ~ LATITUDE, 
                    icon= icons, 
                    popup = popup_sb2, 
                    group = 'Selective Institutions') %>% 
  addLegend(position = 'bottomright',
            title = 'Selectivity',
            pal = ppal2,
            values = SelSchools$Selectivity,
            opacity = 0.7) %>%
  addCircles(data= states_shape_tweets,
             lng = ~lon, lat = ~lat, 
             weight = 2,
             radius = states_shape_tweets$WordCounts*2.8, 
             popup=~popup_sb, 
             color=~ppal(states_shape_tweets$Avg.Sentiment), 
             stroke = TRUE, 
             group = 'Twitter Sentiments',
             fillOpacity = 1) %>% 
  addLegend(position = "bottomleft",
            title = "Average Sentiment Score", 
            pal = ppal, 
            values = states_shape_tweets$Avg.Sentiment, 
            opacity = 0.7) %>% 
  addLayersControl(baseGroups = c("OSM (default)", "Dark"),
                   overlayGroups = c('Twitter Sentiments', 'Selective Institutions'),
                   options = layersControlOptions(collapsed = FALSE), 
                   position = 'bottomright') %>%
  #Add Map Title
  addControl(map_title2, 
             position = 'topright') %>% 
  addResetMapButton() #%>%
  #Add Search 
  # addSearchFeatures(
  #   targetGroups = "Selective Institutions",
  #   options = searchFeaturesOptions(
  #     propertyName = "INSTNM", zoom = 18, openPopup = TRUE, firstTipSubmit = TRUE,
  #     autoCollapse = TRUE, hideMarkerOnCollapse = TRUE )) %>%
  # addControl("<P>Search for Universities<br/>in US by name.</P>",
  #            position = "bottomleft")

```


```{r, echo=FALSE}
library(rsconnect)
#rsconnect::deployApp('grace-xuejing-li-visuals')
```
