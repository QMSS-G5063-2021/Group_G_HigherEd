---
title: "Text_to_Map"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r packages, message = FALSE, warning = FALSE, echo=FALSE}
## Run / Install before executing slides

# Load packages.
library(ggplot2)    # the king of plotting 
library(magrittr)   # chain operators, e.g. to "pipe" a value forward
library(stats)
library(manifestoR)
library(readtext)
library(SnowballC)
library(tidyr)
library(tidytext)      # for data manipulation 
library(qdapDictionaries)
library(base64enc)
#install.packages("ggridges",dependencies =TRUE)
library(tidyverse)
library(RColorBrewer)
#install.packages("wesanderson")
library(wesanderson)
library(rtweet)
library(ggmap)
library(sp)
library(leaflet)
library(tm)
```

```{r, echo=FALSE}
Tweets_state <- readRDS('./Data/Geotweets_state_cleaned.RDS') %>% 
  mutate(id=row_number())
```

#Convert Dataframe to corpus
```{r, eval= TRUE, echo=TRUE}
# Use tm package convert dataframe to corpus: https://www.rdocumentation.org/packages/tm/versions/0.7-8/topics/DataframeSource
doc_id = c(1:4619)
text_df <- data.frame(doc_id, text = Tweets_state$text, stringsAsFactors = FALSE)

# Convert example_text to a corpus: Success_corpus
tweets_corpus <- VCorpus(DataframeSource(text_df))

corpus <- tm_map(tweets_corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, c(stopwords("en")))
corpus <- tm_map(corpus, content_transformer(function(x) gsub("[[:cntrl:]]", "", x))) #remove control characters 
corpus <- tm_map(corpus, content_transformer(function(x) gsub("http\\S+", "", x))) #remove website addresses
corpus <- tm_map(corpus, content_transformer(function(x) gsub("@[A-Za-z0-9]+", "", x))) #remove mentions in text
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)

#Document Term Matrix of tweets
tweets_dtm <- DocumentTermMatrix(corpus)
```

#Convert DTM to tidy dataframe
```{r, eval= TRUE, echo=TRUE}
tweets_tidy <- tidy(tweets_dtm) %>% 
  mutate(index = as.numeric(document)) %>% 
  left_join(Tweets_state, by=c("index"="id"))
```

```{r, echo=FALSE}
#Geocoded Tweets with selected columns
#saveRDS(tweets_tidy, "TidyTwText.RDS")
```

```{r, eval= TRUE, echo=TRUE}
# Get Bing lexicon
bing <- get_sentiments("bing")

# Join text to lexicon
Tweets_bing <- inner_join(tweets_tidy, bing, by = c("term" = "word")) %>%
   # Count by sentiment, index, document
  count(sentiment,index,document, text) %>%
   # Spread sentiments
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive-negative) 

# Review the spread data
Tweets_bing
```