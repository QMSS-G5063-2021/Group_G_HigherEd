---
title: "Text_to_Map"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r packages, message = FALSE, warning = FALSE, echo=FALSE}
## Run / Install before executing slides

# Load packages.
library(ggplot2)    # the king of plotting 
library(magrittr)   # chain operators, e.g. to "pipe" a value forward
library(stats)
library(manifestoR)
library(readtext)
library(SnowballC)
library(tidyr)
library(tidytext)      # for data manipulation 
library(qdapDictionaries)
library(base64enc)
#install.packages("ggridges",dependencies =TRUE)
library(tidyverse)
library(RColorBrewer)
#install.packages("wesanderson")
library(wesanderson)
library(rtweet)
library(ggmap)
library(sp)
library(leaflet)
library(tm)
library(htmlwidgets)
library(htmltools)
#install.packages("leaflet.extras", dependencies = TRUE)
library(leaflet.extras)
library(zoo)
library(wordcloud2)
library(wesanderson)
library(lubridate)
#install webshot
library(webshot)
install_phantomjs()
```

```{r, echo=FALSE}
Tweets_state <- readRDS('~/Documents/GitHub/DataVisualization/Group_G_HigherEd/src/data/twitter_data/Geotweets_state_cleaned.RDS') %>% 
  mutate(id=row_number())

```

```{r, echo=FALSE}
#source("../../src/visuals/ourtheme.R")

SearchTrend <- read_csv('~/Documents/GitHub/DataVisualization/Group_G_HigherEd/src/data/twitter_data/SearchTrend.csv', col_names = TRUE, skip = 2) %>% 
  rename(Total_Searches = 'student loan forgiveness: (United States)') %>% 
  mutate(Month = zoo::as.yearmon(Month))

Trend <- ggplot(SearchTrend,aes(x=Month, y=Total_Searches,group=1)) +
  geom_line(color = "#68A982") +
  scale_color_manual(values="#F2C65F")+
  xlab("") + 
  ylab("Total Searches") + 
  labs(title="Interest in #StudentLoanForgiveness over Time",caption = "Source: Google Trend Search") +
  theme(text=element_text(size=12,  family="serif"),
        panel.background= element_rect(fill="white"),
        legend.position = "bottom",
        axis.ticks = element_blank(),
        plot.title = element_text(hjust=0.5))
Trend  
png("Twitter_Trend.png") #Saving plot to png

dev.off()
```

The Google trend website gives us data about the time and the count of the total number of searches on Google. Plotting the total number of searches overtime helps us to see changes in people's interest in student loan forgiveness. The pattern clearly shows that people's interest in this topic has spiked in 2021. We hypothesized that this is a result of the Biden administration's recent announcement of a reinterpretation of a federal student loan cancellation program which will result in $1 billion in student loan forgiveness. Ever since this announcement, more people have been pushing for a more progressive policy and asking President Biden to cancel student debt through executive action. There is still a lot of uncertainties around whether a universal student loan forgiveness will be initiated via executive order or legislation. 

#Convert Dataframe to corpus
```{r, eval= TRUE, echo=FALSE}
# Use tm package convert dataframe to corpus: https://www.rdocumentation.org/packages/tm/versions/0.7-8/topics/DataframeSource
doc_id = c(1:4619)
text_df <- data.frame(doc_id, text = Tweets_state$text, stringsAsFactors = FALSE)

# Convert example_text to a corpus: Success_corpus
tweets_corpus <- VCorpus(DataframeSource(text_df))

corpus <- tm_map(tweets_corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, c(stopwords("en")))
corpus <- tm_map(corpus, content_transformer(function(x) gsub("[[:cntrl:]]", "", x))) #remove control characters 
corpus <- tm_map(corpus, content_transformer(function(x) gsub("http\\S+", "", x))) #remove website addresses
corpus <- tm_map(corpus, content_transformer(function(x) gsub("@[A-Za-z0-9]+", "", x))) #remove mentions in text
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)

#Document Term Matrix of tweets
tweets_dtm <- DocumentTermMatrix(corpus)
```

#Convert DTM to tidy dataframe
```{r, eval= TRUE, echo=FALSE}
tweets_tidy <- tidy(tweets_dtm) %>% 
  mutate(index = as.numeric(document)) %>% 
  left_join(Tweets_state, by=c("index"="id")) %>% 
    mutate(state = ifelse(state=="new york:long island","new york",
                        ifelse(state=="new york:main","new york",
                        ifelse(state=="new york:main","new york",
                        ifelse(state=="new york:manhattan","new york",
                        ifelse(state=="north carolina:main","north carolina",
                        ifelse(state=="massachusetts:main","massachusetts",
                        ifelse(state=="michigan:north","michigan",
                        ifelse(state=="michigan:south","michigan",
                        ifelse(state=="virginia:main", "virginia",
                        ifelse(state=="washington:main","washington",state))))))))))) 
tweets_tidy_wc <- tweets_tidy %>% 
  group_by(term) %>%
  summarize(n = sum(count)) %>%
  arrange(desc(n)) %>% 
  filter(! term %in% c("student","loan","debt","cancelstudentdebt","amp","forgiveness","like","can","just","get","will")) %>% 
  rename(word=term, freq=n) %>% 
  top_n(1000)

 # Create a wordcloud with wesanderson palette
Twitter_wd <- wordcloud2(tweets_tidy_wc,
       color = wes_palette(name="Royal2"),
       fontFamily = "serif")

Twitter_wd
```

The Wordcloud map shows keywords that appeared in the tweets data. The size of the word represents how frequently it appears in tweets. Some of the most noticeable ones include people, president, loans, job, college, pay, Biden, education, StudentLoanForgiveness, etc. This keywords pattern confirms our initial hypothesis that the recent discussion on canceling student debt on Twitter is around the Biden administration as a result of the $1B student loan cancellation announced in Mid-march.

#Get list of top words by state
```{r, eval= TRUE, echo=FALSE}
State_top_words <- tweets_tidy %>% 
  select(term, count, state) %>% 
  group_by(state, term) %>% 
  summarise(total=sum(count)) %>% 
  arrange(desc(total)) %>% 
  filter(! term %in% c("student","loan","debt","cancelstudentdebt","amp","can","just","will","like")) %>% 
  group_by(state) %>% 
  slice_max(order_by = total, n=10) %>% 
  select(-total) %>% 
  summarise(Terms = list(term))
  
```

```{r, echo=FALSE}
#Geocoded Tweets with selected columns
#saveRDS(tweets_tidy, "TidyTwText.RDS")
```

```{r, eval= TRUE, echo=FALSE}
# Get Bing lexicon
bing <- get_sentiments("bing")

# Join text to lexicon

Tweets_bing <- inner_join(tweets_tidy, bing, by = c("term" = "word")) %>%
   # Count by sentiment, index, document
  count(sentiment,index,document, text) %>%
   # Spread sentiments
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = round(positive-negative,3)) 
```

```{r, eval= TRUE, echo=FALSE, }
# Join sentiment score with original twitter data fame
Tweets_sentiment <- Tweets_state %>% 
  inner_join(Tweets_bing, by = c("id" = "index")) %>% 
  select(-c(text.y,document, size=favorite_count)) 

Pos_Neg <- ggplot(Tweets_sentiment, aes(x=positive,y=negative)) +
  geom_jitter(alpha=0.8, color="rosybrown3") +
  xlab("Positive Sentiment") + 
  ylab("Negative Sentiment") + 
  labs(title="Twitter Sentiment Pattern",caption = "Source: Twitter") +
  theme(text=element_text(size=12,  family="serif"),
        panel.background= element_rect(fill="white"),
        legend.position = "bottom",
        axis.ticks = element_blank(),
        plot.title = element_text(hjust=0.5))

png("Pos_Neg.png") #Saving plot to png
print(Pos_Neg)
dev.off()
```

#Letâ€™s make a choropleth map based on sentiment score by state
```{r, eval= TRUE, echo=FALSE}

# count by state
tw_state <- Tweets_sentiment %>%
  mutate(state = ifelse(state=="new york:long island","new york",
                        ifelse(state=="new york:main","new york",
                        ifelse(state=="new york:main","new york",
                        ifelse(state=="new york:manhattan","new york",
                        ifelse(state=="north carolina:main","north carolina",
                        ifelse(state=="massachusetts:main","massachusetts",
                        ifelse(state=="michigan:north","michigan",
                        ifelse(state=="michigan:south","michigan",
                        ifelse(state=="virginia:main", "virginia",
                        ifelse(state=="washington:main","washington",state))))))))))) %>% 
  group_by(state) %>% 
  mutate(N=n(), T.Sentiment=sum(sentiment), WordCounts = sum(display_text_width)) %>%   #get total number of tweets & sum of sentiment score by state
  select(display_text_width, lon, lat, state, T.Sentiment, N, WordCounts) %>%  #lon & lat are vary within each state.Keep first one. 
  group_by(state, lon, lat) %>% 
  summarise(Avg.Sentiment=round(T.Sentiment/N, 3), WordCounts=WordCounts) %>% #calculate average sentiment score by state and keep total word counts by state
  ungroup(lon,lat) %>% 
  filter(row_number()==1) %>% 
  mutate(lon=ifelse(state=="new york",-73.935242, lon)) %>%  #change coordinates for New York State
  mutate(lat=ifelse(state=="new york",40.730610,lat)) %>% 
  filter(!is.na(state)) %>% 
  left_join(State_top_words, by="state")

# Polygon stuff from shape file
#install.packages("tigris")
library(tigris)
states <- states(cb=T) %>% 
  mutate(name = tolower(NAME))

# Use the Tigris function geo_join to bring together the states shapefile and the tw_states dataframe
states_shape_tweets <- geo_join(states, tw_state, "name", "state")
```

#choropleth map of Twitter Sentiment across States 
```{r, eval= TRUE, echo=FALSE}
# # Creating a color palette based on the number range in the total column
# pal <- colorNumeric("YlOrRd", domain=states_shape_tweets$Avg.Sentiment)
# 
# # Setting up the pop up text
# popup_sb <- paste0("State: ", as.character(states_shape_tweets$NAME),"<br/>",
#                   "Average Sentiment Score: ", as.character(states_shape_tweets$Avg.Sentiment),"<br/>",
#                   "Total Word Count: ",as.character(states_shape_tweets$WordCounts),"<br/>",
#                   "Top Words: ",as.character(states_shape_tweets$Terms),"<br/>")
# 
# # Mapping it with the new tiles CartoDB.Positron
# leaflet() %>%
#   addProviderTiles(providers$CartoDB.Positron) %>%
#   setView(-98.483330, 38.712046, zoom = 4) %>%
#   addPolygons(data = states_shape_tweets ,
#               fillColor = ~pal(states_shape_tweets$Avg.Sentiment),
#               fillOpacity = 0.7,
#               weight = 1.5,
#               opacity = 1,
#               color = "cornsilk",
#               dashArray = "3",
#               smoothFactor = 0.2,
#               popup = ~popup_sb,
#               highlight = highlightOptions(weight = 4,
#                                            color = "#839EA8",
#                                            dashArray = "",
#                                            fillOpacity = 0.7,
#                                            bringToFront = TRUE),
#               popupOptions = popupOptions(style = list("font-weight" = "normal",
#                                                        padding = "3px 8px",
#                                                        "box-shadow" = "3px 3px rgba(0,0,0,0.25)"),
#                                           textsize = "15px",
#                                           direction = "auto")) %>%
#   addLegend(pal = pal,
#             values = states_shape_tweets$Avg.Sentiment,
#             position = "bottomleft",
#             title = "Average Sentiment Score",
#             opacity = 0.7)

```

#Get Locations of Elite/ Selective Schools
```{r, echo=FALSE, warning=FALSE}

Schools <- read_csv('~/Documents/GitHub/DataVisualization/Group_G_HigherEd/src/data/cs_student_debt/2010_2019_student_debt.csv') %>%
  select(INSTNM, CITY,LATITUDE, LONGITUDE, ADM_RATE) %>% 
  filter(!is.na(ADM_RATE))

SelSchools <-Schools %>% 
  mutate(Selectivity = case_when(
    ADM_RATE < 0.1 ~ 'elite',
    ADM_RATE < 0.2 ~ 'highly selective',
    ADM_RATE < 0.3 ~ 'selective',
    ADM_RATE < 0.7 ~ 'less selective',
    TRUE ~ 'not selective')) %>% 
  
  filter(Selectivity %in% c('elite', 'highly selective','selective'), !is.na(LONGITUDE)) %>% 
  distinct(INSTNM, .keep_all=TRUE) 
```


#Twitter Sentiment Map & Institutions'Location Map
```{r, echo=FALSE}
# Creating a color palette based on the number range in the total column
ppal <- colorNumeric("RdYlGn", domain=states_shape_tweets$Avg.Sentiment)

# Creating a color palette based on Selectivity of schools
ppal2 <- colorFactor(palette = c("purple", "forestgreen", "deepskyblue3"), 
               levels = c("elite", "highly selective", "selective"))

icon.pop <- pulseIcons(color = ifelse(SelSchools$Selectivity == "elite", "#E64141", 
                                      ifelse(SelSchools$Selectivity =="highly selective", "#F9C874", "#97D6FF")))

# Setting up the pop up text
popup_sb <- paste0("State: ", as.character(states_shape_tweets$NAME),"<br/>",
                  "Average Sentiment Score: ", as.character(states_shape_tweets$Avg.Sentiment),"<br/>",
                  "Total Word Count: ",as.character(states_shape_tweets$WordCounts),"<br/>",
                  "Top Words: ",as.character(states_shape_tweets$Terms),"<br/>")

popup_sb2 <- paste0("University: ", as.character(SelSchools$INSTNM),"<br/>",
                  "City: ", as.character(SelSchools$CITY),"<br/>",
                  "Admission Rate: ",as.character(SelSchools$ADM_RATE))

getColor <- function(SelSchools) {
  sapply(SelSchools$ADM_RATE, function(ADM_RATE) {
  if(ADM_RATE <= 0.1) {
    "purple"
  } else if(ADM_RATE <= 0.2) {
    "green"
  } else {
    "blue"
  } })
}

icons <- awesomeIcons(
  icon = 'ios-close',
  iconColor = 'black',
  library = 'ion',
  markerColor = getColor(SelSchools)
)


# Map
map_title2 <- tags$p(tags$style('p {color: black; font-size: 14px; family: serif}'),
                    tags$b('#CancelStudentDebt Tweets in the US - Location of Selective Insitutions'))

leaflet() %>%
  setView(-98.483330, 38.712046, zoom = 4) %>% 
  addTiles('http://{s}.basemaps.cartocdn.com/dark_all/{z}/{x}/{y}.png', 
    attribution='Map tiles by <a href="http://stamen.com">Stamen Design</a>, <a href="http://creativecommons.org/licenses/by/3.0">CC BY 3.0</a> &mdash; Map data &copy; <a href="http://www.openstreetmap.org/copyright">OpenStreetMap</a>', group = "Dark") %>%
#Overlay Group  
  # Add Institution Data
  # addPulseMarkers(data = SelSchools,
  #            lng= ~LONGITUDE, lat = ~LATITUDE,
  #            icon = icon.pop,
  #            label = popup_sb2,
  #            group = 'Selective Institutions') %>%
  # addCircles(data = SelSchools,
  #            lng = ~LONGITUDE, lat = ~LATITUDE,
  #            popup=popup_sb2,
  #            radius = SelSchools$ADM_RATE*400, 
  #            color=color_Selectivity,
  #            group = 'Selective Institutions') %>%
  addAwesomeMarkers(data = SelSchools,
                    lng = ~LONGITUDE, lat = ~ LATITUDE, 
                    icon= icons, 
                    popup = popup_sb2, 
                    group = 'Selective Institutions') %>% 
  addLegend(position = 'bottomright',
            title = 'Selectivity',
            pal = ppal2,
            values = SelSchools$Selectivity,
            opacity = 0.7) %>%
  addCircles(data= states_shape_tweets,
             lng = ~lon, lat = ~lat, 
             weight = 2,
             radius = states_shape_tweets$WordCounts*2.8, 
             popup=~popup_sb, 
             color=~ppal(states_shape_tweets$Avg.Sentiment), 
             stroke = TRUE, 
             group = 'Twitter Sentiments',
             fillOpacity = 0.8) %>% 
  addLegend(position = "bottomleft",
            title = "Average Sentiment Score", 
            pal = ppal, 
            values = states_shape_tweets$Avg.Sentiment, 
            opacity = 0.7) %>% 
  addLayersControl( 
          overlayGroups =c("Twitter Sentiments", "Selective Institutions"),
          options = layersControlOptions(collapsed=FALSE),
          position = "bottomright") %>% 
  #Add Map Title
  addControl(map_title2, 
             position = 'topright') %>% 
  addResetMapButton() %>%
  #Add Search 
  # addSearchFeatures(
  #   targetGroups = "Selective Institutions",
  #   options = searchFeaturesOptions(
  #     propertyName = "INSTNM", zoom = 18, openPopup = TRUE, firstTipSubmit = TRUE,
  #     autoCollapse = TRUE, hideMarkerOnCollapse = TRUE )) %>%
  # addControl("<P>Search for Universities<br/>in US by name.</P>",
  #            position = "bottomleft")   
  addPolylines(data = states_shape_tweets, color = "lightgrey", opacity = 0.2, weight = 2)

```

   
   Lastly, we want to see if there's an overlap between the states where student loan forgiveness is more discussed and the locations of the selective schools. We added two layers on the map to showcase patterns. The first layer tells us which states are most concerned about student loan forgiveness. The larger the circle, the higher the number of total tweets word count. The gradient color scale shows the sentiment score of each state, where red means a more negative average tweet sentiment and green means a more positive tweet sentiment on average. In addition, the popup window shows the top tweet keywords in each state after we got rid of the most frequent keywords such as student, loan, debt, Cancelstudentdebt, and amp. People tweet the most about student loan forgiveness in California. Other areas where there are more discussions on student loan forgiveness are states along the coastlines. Midwestern states are least concerned about student loan forgiveness.
   
   The second layer shows where the selective institutions are. Elite universities are in purple, highly selective universities are in green, and the blue markers are selective universities. Overall, these selective universities concentrate along the coastlines. On the west coast, most of the selective universities are in California. The selective universities spread out more evenly on the east coast. Overall, we can see an overlap by looking at the two layers together. States with a higher concentration of selective schools are also where student loan forgiveness is the most discussed. 
   
# Shiny
```{r, echo=FALSE}
library(rsconnect)
#rsconnect::deployApp('grace-xuejing-li-visuals')
```
